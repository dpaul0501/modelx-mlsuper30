
# ğŸ“˜ ModelX-ML Bootcamp â€“ Week X: Basics of Machine Learning

Welcome to **Week X** of the **ModelX-ML 30-Week Bootcamp**, where we dive into the foundational principles that power modern machine learning algorithms.

---

## ğŸ§  Topics Covered

This week focuses on the **core statistical and mathematical ideas** behind machine learning models. The content is inspired by:

- **Cornell CS4780 (2018 Fall) â€“ Intro to Machine Learning**  
  ğŸ”— [Course Syllabus](https://www.cs.cornell.edu/courses/cs4780/2018fa/syllabus/)

- **Stanford CS229 â€“ Lecture 1**  
  ğŸ“š [Lecture Notes: Linear & Logistic Regression](http://cs229.stanford.edu/notes2023fall/cs229-notes1.pdf)

---

## ğŸ—‚ï¸ Lecture Summary

### ğŸ”¹ 1. Maximum Likelihood Estimation (MLE)
- **Goal:** Estimate parameters that maximize the probability of observing the given data.
- **Assumes:** Data is drawn from a known parametric distribution.
- **Example:** Estimating the mean of a Gaussian distribution from sample data.

### ğŸ”¹ 2. Maximum A Posteriori Estimation (MAP)
- **Goal:** Incorporate prior belief into parameter estimation.
- **Builds on:** MLE, but adds a **Bayesian prior**.
- **Example:** MAP becomes MLE when using a uniform prior.

### ğŸ”¹ 3. Linear Regression
- **Model:** \( y = \theta^T x + \epsilon \)
- **Assumptions:** Noise \( \epsilon \sim \mathcal{N}(0, \sigma^2) \)
- **Optimization:** Minimizing squared error using closed-form or gradient descent.

### ğŸ”¹ 4. Logistic Regression
- **Model:** \( P(y=1|x) = \sigma(\theta^T x) \), where \( \sigma \) is the sigmoid.
- **Loss Function:** Cross-entropy (log-loss).
- **Use Case:** Binary classification tasks.

---

## ğŸ“Œ Key Takeaways

- **MLE vs MAP**: MLE is purely data-driven, MAP blends data with prior belief.
- **Regression vs Classification**: Linear regression is for continuous outputs, logistic regression handles discrete (binary) outcomes.
- **Statistical grounding**: Understanding the **probabilistic interpretations** behind models enables better model design and debugging.

---

## ğŸ”§ Practice Ideas

- Derive the MLE for the mean and variance of a normal distribution.
- Compare MAP vs MLE estimations with small datasets.
- Implement linear regression using both closed-form and gradient descent.
- Train logistic regression on a simple dataset (e.g., breast cancer or Titanic) and visualize decision boundaries.

---

## ğŸ“š Suggested Readings

- CS4780 Lecture Notes: Chapters on Estimation Theory & Regression
- CS229 Lecture 1 PDF: [Link](http://cs229.stanford.edu/notes2023fall/cs229-notes1.pdf)
- Bishopâ€™s *Pattern Recognition and Machine Learning* â€“ Chapters 1â€“3

---

## ğŸ§­ Next Week Preview: Recommendation Systems

Next week, weâ€™ll dive into **Recommendation Systems**!  
We'll explore:
- Popularity-based and collaborative filtering approaches  
- User-item matrix factorization  
- Embedding-based methods  
- Real-world use cases in Netflix, Amazon, and Spotify

Get ready to personalize at scale! ğŸ¯
